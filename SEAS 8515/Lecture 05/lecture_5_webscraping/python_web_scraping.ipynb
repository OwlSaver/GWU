{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "623978db-df41-4608-b910-632fc336e5b2",
   "metadata": {},
   "source": [
    "# Python Web-Scraping - An Introduction\n",
    "\n",
    "## Topics\n",
    "\n",
    "- Web basics\n",
    "- Making web requests\n",
    "- Inspecting web sites\n",
    "- Retrieving JSON data\n",
    "- Using Xpaths to retrieve html content\n",
    "- Parsing html content\n",
    "- Cleaning and storing text from html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ad7628-7577-4937-92f6-316828f44071",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Assumes knowledge of Python, including:\n",
    "- Lists\n",
    "- Dictionaries\n",
    "- Logical indexing\n",
    "- Iteration with for-loops\n",
    "\n",
    "Assumes basic knowledge of web page structure.\n",
    "\n",
    "## Goals\n",
    "\n",
    "This workshop is organized into two main parts:\n",
    "1. Retrieve information in JSON format.\n",
    "2. Parse HTML files.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81afef82-5c77-41ac-a8e0-7e8f11348511",
   "metadata": {},
   "source": [
    "## Web Scraping Background\n",
    "\n",
    "### What is web scraping?\n",
    "Web scraping is the activity of automating the retrieval of information from a web service designed for human interaction.\n",
    "\n",
    "### Is web scraping legal? Is it ethical?\n",
    "It depends. Legal aspects vary, so if you have legal questions, seek legal counsel. Ethically, you can mitigate issues by building delays and restrictions into your web scraping program to avoid impacting the availability of the web service for other users or the hosting costs for the service provider.\n",
    "\n",
    "### Web Scraping Approaches\n",
    "No two websites are identical — websites are built for different purposes by different people and thus have different underlying structures. Because they are heterogeneous, there is no single way to scrape a website. The scraping approach must be tailored to each individual site. Commonly used approaches include:\n",
    "- Using requests to extract information from structured JSON files.\n",
    "- Using requests to extract information from HTML.\n",
    "- Automating a browser to retrieve information from HTML.\n",
    "\n",
    "Remember, even once you've decided upon the best approach for a particular site, it will be necessary to modify that approach to suit your specific use-case.\n",
    "\n",
    "### How does the web work?\n",
    "#### Components\n",
    "- **Clients** are the typical web user’s internet-connected devices (e.g., your computer connected to your Wi-Fi) and web-accessing software available on those devices (usually a web browser like Firefox or Chrome).\n",
    "- **Servers** are computers that store webpages, sites, or apps. When a client device wants to access a webpage, a copy of the webpage is downloaded from the server onto the client machine to be displayed in the user’s web browser.\n",
    "- **HTTP** is the language for clients and servers to speak to each other.\n",
    "\n",
    "#### Process\n",
    "When you type a web address into your browser:\n",
    "1. The browser finds the address of the server that the website lives on.\n",
    "2. The browser sends an HTTP request message to the server, asking it to send a copy of the website to the client.\n",
    "3. If the server approves the client’s request, the server sends the client a \"200 OK\" message, and then the website starts displaying in the browser.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da1a1e0-23e6-4e71-9c50-6cbf7a5bc46a",
   "metadata": {},
   "source": [
    "## Goal: Retrieve Data in JSON Format\n",
    "\n",
    "The objective is to retrieve information in JSON format and organize it into a spreadsheet. Below are the steps we will follow to achieve this:\n",
    "\n",
    "### Steps to Retrieve and Organize Data\n",
    "\n",
    "1. **Inspect the Website**:\n",
    "   - Check if the content at [Harvard Art Museums Collections](https://www.harvardartmuseums.org/collections) is stored in JSON format.\n",
    "\n",
    "2. **Make a Request**:\n",
    "   - Send a request to the website server to retrieve the JSON file. This involves using tools like `requests` in Python to access the data.\n",
    "\n",
    "3. **Convert JSON to Dictionary**:\n",
    "   - Once the JSON data is retrieved, convert it from JSON format into a Python dictionary using Python’s json library. This step is crucial for manipulating the data in Python.\n",
    "\n",
    "4. **Extract Data and Store in CSV**:\n",
    "   - Extract the necessary data from the Python dictionary and store it in a .csv file. This is done using Python’s csv library or pandas DataFrame to format and save the data.\n",
    "   \n",
    "![json-format.png](json-format.png)\n",
    "\n",
    "### Understanding the Website's Backend\n",
    "\n",
    "Like most modern web pages, a lot goes on behind the scenes at the Harvard Art Museums website to produce the page we see in our browser. By understanding how the website works when we interact with it, we can begin to retrieve data effectively.\n",
    "\n",
    "If we are lucky, we'll find a resource on the website that returns the data we're looking for in a structured format like JSON. This is advantageous as it simplifies the process of converting data from JSON into a spreadsheet format such as CSV or Excel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0139204b-a049-4813-84ab-3074e34f379d",
   "metadata": {},
   "source": [
    "## Examine the Website's Structure for Data Retrieval\n",
    "\n",
    "### Basic Strategy for Web Scraping\n",
    "\n",
    "The approach to web scraping generally follows a consistent process across different projects. We will use a web browser (Chrome or Firefox recommended) to examine the page from which we wish to retrieve data. The key is to use the developer tools available in the browser to inspect the webpage and identify how data is loaded and presented.\n",
    "\n",
    "### Step-by-Step Process\n",
    "\n",
    "1. **Open the Collections Web Page**:\n",
    "   - Begin by navigating to the collections page on a web browser and open the developer tools. This is typically done by right-clicking on the page and selecting \"Inspect\" or pressing `Ctrl+Shift+I` on your keyboard.\n",
    "\n",
    "2. **Using Network Tools**:\n",
    "   - Within the developer tools, switch to the \"Network\" tab. This tab is crucial as it displays network requests made by your browser to the server.\n",
    "\n",
    "3. **Interact with the Page**:\n",
    "   - Scroll to the bottom of the Collections page and click on the “Load More” button. Observe the network activity that occurs when you click the button.\n",
    "\n",
    "4. **Analyze the Requests**:\n",
    "   - A list of HTTP requests will appear in the Network tab when you click the “Load More Collections” button. Review these requests to identify which one carries the data you need.\n",
    "\n",
    "5. **Identify Data Retrieval Method**:\n",
    "   - Pay attention to the second request made to a script named 'browse'. This request returns information in JSON format, which is what we need for scraping. \n",
    "   \n",
    "![harvardart_1.png](harvardart_1.png)\n",
    "\n",
    "![harvardart_2.png](harvardart_2.png)\n",
    "\n",
    "### Retrieving Data\n",
    "\n",
    "- **Endpoint for Collection Data**:\n",
    "  - To retrieve the collection data, make GET requests to `https://www.harvardartmuseums.org/browse` with the correct parameters. This will fetch the data in JSON format, which can then be processed or converted into the desired format (e.g., CSV).\n",
    "\n",
    "By following these steps, you can start to retrieve data from web pages that load additional content dynamically, such as through the \"Load More\" buttons or infinite scroll functionalities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2371bc-527a-4c44-93a7-739d8565acc4",
   "metadata": {},
   "source": [
    "## Making Requests\n",
    "\n",
    "To effectively retrieve information from a website, understanding the structure of the URL, or \"web address,\" is crucial. This allows us to specify the location of the resources we want to collect, such as web pages.\n",
    "\n",
    "### Understanding URL Structure\n",
    "\n",
    "A URL is typically composed of several parts:\n",
    "\n",
    "1. **Protocol**: The method of access (e.g., `https`, `http`).\n",
    "2. **Domain**: The central web address of the site (e.g., `www.harvardartmuseums.org`).\n",
    "3. **Path**: The specific address within the domain where resources are located (e.g., `/browse`).\n",
    "4. **Parameters (Query String)**: Additional instructions for the server about what exactly to return, often in key-value pairs (e.g., `load_amount=10&offset=0`).\n",
    "5. **Fragment**: An internal page reference that directs the browser to a specific part of the page (optional and may not be present)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f23c270-c465-4575-b1af-e5ebb6cf8768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.harvardartmuseums.org/browse\n"
     ]
    }
   ],
   "source": [
    "museum_domain = 'https://www.harvardartmuseums.org'\n",
    "collection_path = 'browse'\n",
    "\n",
    "collection_url = (museum_domain\n",
    "                  + \"/\"\n",
    "                  + collection_path)\n",
    "\n",
    "print(collection_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7caa0e94-cd4a-471e-8911-bef8331656cb",
   "metadata": {},
   "source": [
    "\n",
    "### Practical Tips for URL Management\n",
    "\n",
    "- **Variable Usage**: It’s practical to create variables for commonly used domains and paths. This simplifies the process of changing out paths and parameters when needed.\n",
    "- **Syntax Details**:\n",
    "  - The path is separated from the domain by a `/`.\n",
    "  - Parameters are appended to the URL after the path and start with a `?`.\n",
    "  - If multiple parameters are used, they are separated by `&`.\n",
    "\n",
    "Understanding these URL components and their structure helps in crafting precise requests to retrieve data from web servers efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0db1111c-f68c-4960-a1ca-ef227620810f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "collections1 = requests.get(\n",
    "    collection_url,\n",
    "    params = {'load_amount': 10,\n",
    "                  'offset': 0}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6472097e-10d0-464d-a6ca-fd7a0cb266c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "collections1 = collections1.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f8f3e3e-5b5a-4255-b7ab-61fa9211ff4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(collections1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91496c0e-f173-4751-879a-75969e57abc0",
   "metadata": {},
   "source": [
    "That’s it. Really, we are done here. Everyone go home!\n",
    "\n",
    "OK not really, there is still more we can learn. But you have to admit that was pretty easy. If you can identify a service that returns the data you want in structured from, web scraping becomes a pretty trivial enterprise. We’ll discuss several other scenarios and topics, but for some web scraping tasks this is really all you need to know."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15574dc-bd27-4fd9-bc1f-8cbf1c398bac",
   "metadata": {},
   "source": [
    "## Organizing & saving the data\n",
    "The records we retrieved from https://www.harvardartmuseums.org/browse are arranged as a list of dictionaries. We can easily select the fields of arrange these data into a pandas DataFrame to facilitate subsequent analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14e6ba9d-bbfa-4781-8d04-a589df6a5216",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "records1 = pd.DataFrame.from_records(collections1['records'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "52f37661-4ca1-45b6-8e9a-520a929f2dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#records1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436db08f-ff12-45e8-ba84-c258bbf35d9a",
   "metadata": {},
   "source": [
    "## Iterating to retrieve all the data\n",
    "Of course we don’t want just the first page of collections. How can we retrieve all of them?\n",
    "\n",
    "Now that we know the web service works, and how to make requests in Python, we can iterate in the usual way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66eeae8f-18f7-4676-a337-822096cb53b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "records = []\n",
    "for offset in range(0, 50, 10):\n",
    "    param_values = {'load_amount': 10, 'offset': offset}\n",
    "    current_request = requests.get(collection_url, params = param_values)\n",
    "    records.extend(current_request.json()['records'])\n",
    "## convert list of dicts to a `DataFrame`\n",
    "records_final = pd.DataFrame.from_records(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0bd92649-d862-4a1b-b3e0-b7c640eff4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#records_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78acb9ff-1220-4d5a-bd9f-dad5e55e8991",
   "metadata": {},
   "source": [
    "## Retrieve exhibits data\n",
    "\n",
    "In this exercise, we will retrieve information about the art exhibitions at Harvard Art Museums from https://www.harvardartmuseums.org/exhibitions\n",
    "\n",
    "Using a web browser (Firefox or Chrome recommended) inspect the page at https://www.harvardartmuseums.org/exhibitions. Examine the network traffic as you interact with the page. Try to find where the data displayed on that page comes from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f80ff945-1c8f-4348-9a0a-8124446a1efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.harvardartmuseums.org/search/load_more\n"
     ]
    }
   ],
   "source": [
    "museum_domain = \"https://www.harvardartmuseums.org\"\n",
    "exhibit_path = \"search/load_more\"\n",
    "exhibit_url = museum_domain + \"/\" + exhibit_path\n",
    "print(exhibit_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5f72dc-4420-4340-964f-6ab2560d9b16",
   "metadata": {},
   "source": [
    "Make a get request in Python to retrieve the data from the URL identified in step1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "11528abc-f25c-443a-9468-55f4abd6cb65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'application/json'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from pprint import pprint as print \n",
    "exhibit1 = requests.get(exhibit_url, params = {'type': 'past-exhibition', 'page': 1})\n",
    "print(exhibit1.headers[\"Content-Type\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9dba3518-3ddb-4b39-8857-1952596a6600",
   "metadata": {},
   "outputs": [],
   "source": [
    "exhibit = exhibit1.json()\n",
    "#print(exhibit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "28c1c27f-51e0-4096-9aef-aff766ed56eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base URL of the exhibition section\n",
    "base_url = \"https://www.harvardartmuseums.org/browse\"\n",
    "\n",
    "# List to hold all records from the first five pages\n",
    "firstFivePages = []\n",
    "\n",
    "# Loop through the first five pages\n",
    "for page in range(1, 6):\n",
    "    # Parameters for the GET request\n",
    "    params = {\n",
    "        'type': 'past-exhibition',\n",
    "        'page': page\n",
    "    }\n",
    "    \n",
    "    # Make the GET request\n",
    "    response = requests.get(base_url, params=params)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Convert JSON response to Python dictionary\n",
    "        data = response.json()\n",
    "        \n",
    "        # Check if 'records' key is in the JSON data\n",
    "        if 'records' in data:\n",
    "            # Extend the list with records\n",
    "            firstFivePages.extend(data['records'])\n",
    "        else:\n",
    "            print(f\"No records found on page {page}\")\n",
    "    else:\n",
    "        print(f\"Failed to retrieve data for page {page}: {response.status_code}\")\n",
    "\n",
    "# Create a DataFrame from the list of records\n",
    "firstFivePages_records = pd.DataFrame(firstFivePages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e465c7bb-2324-4358-91c9-0bbd132b9041",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['copyright', 'contextualtextcount', 'creditline', 'accesslevel',\n",
       "       'dateoflastpageview', 'classificationid', 'division', 'markscount',\n",
       "       'publicationcount', 'totaluniquepageviews', 'contact', 'colorcount',\n",
       "       'rank', 'id', 'state', 'verificationleveldescription', 'period',\n",
       "       'images', 'worktypes', 'imagecount', 'totalpageviews', 'accessionyear',\n",
       "       'standardreferencenumber', 'signed', 'classification', 'relatedcount',\n",
       "       'verificationlevel', 'primaryimageurl', 'titlescount', 'peoplecount',\n",
       "       'style', 'lastupdate', 'commentary', 'periodid', 'technique', 'edition',\n",
       "       'description', 'medium', 'lendingpermissionlevel', 'title',\n",
       "       'accessionmethod', 'colors', 'provenance', 'groupcount', 'dated',\n",
       "       'department', 'dateend', 'people', 'url', 'dateoffirstpageview',\n",
       "       'century', 'objectnumber', 'labeltext', 'datebegin', 'culture',\n",
       "       'exhibitioncount', 'imagepermissionlevel', 'mediacount', 'objectid',\n",
       "       'techniqueid', 'dimensions', 'seeAlso', 'details'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "firstFivePages_records.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fa0d26-a48a-4351-8636-cad336498fbc",
   "metadata": {},
   "source": [
    "## Document Object Model (DOM)\n",
    "\n",
    "### Understanding DOM\n",
    "\n",
    "The Document Object Model (DOM) is crucial for working with HTML or XML documents programmatically. It provides a structured tree representation of the document, allowing developers to navigate and modify the content effectively.\n",
    "\n",
    "### Features of DOM\n",
    "\n",
    "- **Tree Structure**: The DOM represents an HTML or XML document as a tree structure where each node is an object representing part of the document.\n",
    "- **Language-Independent**: It is a cross-platform and language-independent interface, making it a standard tool for web development across different programming environments.\n",
    "- **Nodes and Objects**: Each branch of the tree ends in a node, and each node can contain objects like elements, attributes, and text.\n",
    "\n",
    "### Manipulating DOM\n",
    "\n",
    "- **Programmatic Access**: DOM methods provide programmatic access to the tree, enabling changes to the document’s structure, style, and content.\n",
    "- **Dynamic Interactions**: This allows web pages to be dynamic, as scripts can react to user events, modify the DOM, and update the display without needing to reload the page.\n",
    "\n",
    "   \n",
    "![dom_webscraping.png](dom_webscraping.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9277fdf5-c855-4e67-8bbf-54c514aacc94",
   "metadata": {},
   "source": [
    "## Retrieving HTML\n",
    "When I inspect the network traffic while interacting with https://www.harvardartmuseums.org/calendar I don’t see any requests that return JSON data. The best we can do appears to be to return HTML.\n",
    "\n",
    "To retrieve data on the events listed in the calender, the first step is the same as before: we make a get request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d13adc0b-ea2c-418b-8552-8212d625facd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'https://www.harvardartmuseums.org/calendar'\n"
     ]
    }
   ],
   "source": [
    "calendar_path = 'calendar'\n",
    "\n",
    "calendar_url = (museum_domain # recall that we defined museum_domain earlier\n",
    "                  + \"/\"\n",
    "                  + calendar_path)\n",
    "\n",
    "print(calendar_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "990f10a6-9a0c-4818-93f9-0c83fdc6f126",
   "metadata": {},
   "outputs": [],
   "source": [
    "events = requests.get(calendar_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "815a28c6-5a1c-4753-a447-4b555da6c108",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'text/html; charset=UTF-8'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "events.headers['Content-Type']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634b029d-4d56-4d67-be88-d5e809b8cae2",
   "metadata": {},
   "source": [
    "## Scrapy: for large / complex projects\n",
    "Scraping websites using the requests library to make GET and POST requests, and the lxml library to process HTML is a good way to learn basic web scraping techniques. It is a good choice for small to medium size projects. For very large or complicated scraping tasks the scrapy library offers a number of conveniences, including asynchronous retrieval, session management, convenient methods for extracting and storing values, and more. More information about scrapy can be found at https://doc.scrapy.org.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c013b202-a05c-45df-aaa9-8b65b034cd00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
