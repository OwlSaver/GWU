{"cells":[{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1695257365549,"user":{"displayName":"Rami Elyoussef","userId":"18406037533613543434"},"user_tz":300},"id":"7rfqJ3RGna4x","outputId":"1ef3daa3-9766-45fd-ed57-493da4af6474"},"outputs":[{"name":"stdout","output_type":"stream","text":["Starting gradient descent with learning rate = 1\n","Iteration 1: w = 8, Gradient = -8\n","Iteration 2: w = 0, Gradient = 8\n","Iteration 3: w = 8, Gradient = -8\n","Iteration 4: w = 0, Gradient = 8\n","Iteration 5: w = 8, Gradient = -8\n","Iteration 6: w = 0, Gradient = 8\n","Iteration 7: w = 8, Gradient = -8\n","Iteration 8: w = 0, Gradient = 8\n","Iteration 9: w = 8, Gradient = -8\n","Iteration 10: w = 0, Gradient = 8\n","\n","Starting gradient descent with learning rate = 0.1\n","Iteration 1: w = 0.8, Gradient = -8\n","Iteration 2: w = 1.4400000000000002, Gradient = -6.4\n","Iteration 3: w = 1.952, Gradient = -5.119999999999999\n","Iteration 4: w = 2.3616, Gradient = -4.096\n","Iteration 5: w = 2.68928, Gradient = -3.2767999999999997\n","Iteration 6: w = 2.9514240000000003, Gradient = -2.6214399999999998\n","Iteration 7: w = 3.1611392, Gradient = -2.0971519999999995\n","Iteration 8: w = 3.32891136, Gradient = -1.6777216\n","Iteration 9: w = 3.463129088, Gradient = -1.3421772799999996\n","Iteration 10: w = 3.5705032704, Gradient = -1.0737418239999998\n","Iteration 11: w = 3.6564026163200003, Gradient = -0.8589934591999997\n","Iteration 12: w = 3.725122093056, Gradient = -0.6871947673599994\n","Iteration 13: w = 3.7800976744448, Gradient = -0.5497558138879999\n","Iteration 14: w = 3.82407813955584, Gradient = -0.4398046511103999\n","Iteration 15: w = 3.8592625116446717, Gradient = -0.3518437208883203\n","Iteration 16: w = 3.8874100093157375, Gradient = -0.2814749767106566\n","Iteration 17: w = 3.90992800745259, Gradient = -0.2251799813685249\n","Iteration 18: w = 3.927942405962072, Gradient = -0.1801439850948201\n","Iteration 19: w = 3.9423539247696575, Gradient = -0.14411518807585644\n","Iteration 20: w = 3.953883139815726, Gradient = -0.11529215046068497\n","Iteration 21: w = 3.9631065118525806, Gradient = -0.09223372036854816\n","Iteration 22: w = 3.9704852094820646, Gradient = -0.07378697629483888\n","Iteration 23: w = 3.9763881675856516, Gradient = -0.05902958103587075\n","Iteration 24: w = 3.981110534068521, Gradient = -0.047223664828696776\n","Iteration 25: w = 3.984888427254817, Gradient = -0.037778931862957776\n","Iteration 26: w = 3.9879107418038537, Gradient = -0.030223145490365866\n","Iteration 27: w = 3.990328593443083, Gradient = -0.024178516392292515\n","Iteration 28: w = 3.9922628747544664, Gradient = -0.019342813113834012\n","Iteration 29: w = 3.993810299803573, Gradient = -0.01547425049106721\n","Iteration 30: w = 3.995048239842858, Gradient = -0.012379400392854123\n","Iteration 31: w = 3.9960385918742864, Gradient = -0.009903520314283654\n","Iteration 32: w = 3.9968308734994293, Gradient = -0.0079228162514271\n","Iteration 33: w = 3.9974646987995435, Gradient = -0.006338253001141325\n","Iteration 34: w = 3.997971759039635, Gradient = -0.00507060240091306\n","Iteration 35: w = 3.998377407231708, Gradient = -0.0040564819207302705\n","Iteration 36: w = 3.998701925785366, Gradient = -0.0032451855365840387\n","Iteration 37: w = 3.998961540628293, Gradient = -0.0025961484292675863\n","Iteration 38: w = 3.9991692325026342, Gradient = -0.0020769187434144243\n","Iteration 39: w = 3.9993353860021075, Gradient = -0.0016615349947315394\n","Iteration 40: w = 3.999468308801686, Gradient = -0.001329227995785054\n","Iteration 41: w = 3.9995746470413485, Gradient = -0.0010633823966283984\n","Iteration 42: w = 3.9996597176330786, Gradient = -0.000850705917303074\n","Iteration 43: w = 3.999727774106463, Gradient = -0.0006805647338428145\n","Iteration 44: w = 3.99978221928517, Gradient = -0.0005444517870740739\n","Iteration 45: w = 3.9998257754281363, Gradient = -0.0004355614296596144\n","Iteration 46: w = 3.999860620342509, Gradient = -0.00034844914372733626\n","Iteration 47: w = 3.999888496274007, Gradient = -0.0002787593149822243\n","Iteration 48: w = 3.9999107970192056, Gradient = -0.00022300745198577943\n","Iteration 49: w = 3.9999286376153647, Gradient = -0.00017840596158880118\n","Iteration 50: w = 3.9999429100922916, Gradient = -0.00014272476927068567\n","Iteration 51: w = 3.999954328073833, Gradient = -0.00011417981541672617\n","Iteration 52: w = 3.9999634624590668, Gradient = -9.134385233355857e-05\n","Iteration 53: w = 3.9999707699672533, Gradient = -7.307508186649159e-05\n","Iteration 54: w = 3.999976615973803, Gradient = -5.8460065493370905e-05\n","Iteration 55: w = 3.9999812927790424, Gradient = -4.676805239434145e-05\n","Iteration 56: w = 3.9999850342232337, Gradient = -3.7414441915295527e-05\n","Iteration 57: w = 3.999988027378587, Gradient = -2.9931553532591693e-05\n","Iteration 58: w = 3.9999904219028695, Gradient = -2.394524282589572e-05\n","Iteration 59: w = 3.9999923375222957, Gradient = -1.9156194261071846e-05\n","Iteration 60: w = 3.9999938700178364, Gradient = -1.532495540867984e-05\n","Iteration 61: w = 3.999995096014269, Gradient = -1.2259964327299144e-05\n","Iteration 62: w = 3.9999960768114153, Gradient = -9.80797146166168e-06\n","Iteration 63: w = 3.9999968614491324, Gradient = -7.846377169329344e-06\n","Iteration 64: w = 3.999997489159306, Gradient = -6.277101735285839e-06\n","Iteration 65: w = 3.9999979913274446, Gradient = -5.021681388051036e-06\n","Iteration 66: w = 3.9999983930619556, Gradient = -4.0173451107961e-06\n","Iteration 67: w = 3.9999987144495646, Gradient = -3.2138760888145157e-06\n","Iteration 68: w = 3.9999989715596516, Gradient = -2.571100870873977e-06\n","Iteration 69: w = 3.9999991772477212, Gradient = -2.056880696876817e-06\n","Iteration 70: w = 3.999999341798177, Gradient = -1.6455045575014537e-06\n","Iteration 71: w = 3.9999994734385416, Gradient = -1.3164036456458916e-06\n","Iteration 72: w = 3.9999995787508333, Gradient = -1.0531229168719847e-06\n","Iteration 73: w = 3.9999996630006667, Gradient = -8.424983333199521e-07\n","Iteration 74: w = 3.9999997304005332, Gradient = -6.739986666559616e-07\n","Iteration 75: w = 3.9999997843204267, Gradient = -5.39198933502405e-07\n","Iteration 76: w = 3.9999998274563415, Gradient = -4.313591466242883e-07\n","Iteration 77: w = 3.9999998619650734, Gradient = -3.450873169441593e-07\n","Iteration 78: w = 3.9999998895720585, Gradient = -2.7606985320005606e-07\n","Iteration 79: w = 3.999999911657647, Gradient = -2.2085588291531622e-07\n","Iteration 80: w = 3.9999999293261177, Gradient = -1.766847059769816e-07\n","Iteration 81: w = 3.999999943460894, Gradient = -1.413477646039496e-07\n","Iteration 82: w = 3.9999999547687155, Gradient = -1.1307821168315968e-07\n","Iteration 83: w = 3.9999999638149726, Gradient = -9.046256899125638e-08\n","Iteration 84: w = 3.9999999710519782, Gradient = -7.237005483773373e-08\n","Iteration 85: w = 3.9999999768415826, Gradient = -5.789604351491562e-08\n","Iteration 86: w = 3.999999981473266, Gradient = -4.6316834811932495e-08\n","Iteration 87: w = 3.9999999851786128, Gradient = -3.7053467849545996e-08\n","Iteration 88: w = 3.99999998814289, Gradient = -2.964277445727248e-08\n","Iteration 89: w = 3.999999990514312, Gradient = -2.3714219921089352e-08\n","Iteration 90: w = 3.9999999924114498, Gradient = -1.8971375581600114e-08\n","Iteration 91: w = 3.9999999939291597, Gradient = -1.517710046528009e-08\n","Iteration 92: w = 3.9999999951433276, Gradient = -1.2141680549859757e-08\n","Iteration 93: w = 3.999999996114662, Gradient = -9.713344795159173e-09\n","Converged!\n","\n"]}],"source":["# Function to calculate the gradient of the cost function\n","def gradient(w):\n","    return 2 * (w - 4)\n","\n","# Function to perform gradient descent\n","def gradient_descent(learning_rate, max_iterations):\n","    w = 0  # Starting point\n","    print(f\"Starting gradient descent with learning rate = {learning_rate}\")\n","    for i in range(max_iterations):\n","        old_w = w  # Store the old value of w\n","        grad = gradient(w)  # Calculate the gradient at current w\n","        w = w - learning_rate * grad  # Update w\n","\n","        # Print update\n","        print(f\"Iteration {i + 1}: w = {w}, Gradient = {grad}\")\n","\n","        # Check for convergence\n","        if abs(w - old_w) < 0.000000001:\n","            print(\"Converged!\")\n","            break\n","    print()\n","\n","# Perform gradient descent for the first scenario (learning rate = 1)\n","# Here, we won't likely see convergence due to the high learning rate\n","gradient_descent(1, 10)\n","\n","# Perform gradient descent for the second scenario (learning rate = 0.1)\n","# Here, we should see convergence\n","gradient_descent(0.1, 500)\n"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPmHXl0PpYceZLL5XtWoM7r","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":0}
