{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGVq6H8P8Tez"
      },
      "source": [
        "# Homework Assignment #7: Autoencoders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pT70YGx1B4B6"
      },
      "source": [
        "# Part 1: Autoencoders\n",
        "\n",
        "In this assignment, you will implement a simple autoencoder.\n",
        "\n",
        "The provided script creates and trains an autoencoder using the MNIST dataset, which is a popular dataset for classification tasks.\n",
        "\n",
        "Fill in the code if indicated with the comment \"PUT YOUR CODE HERE\" and follow all the steps in the document.\n",
        "\n",
        "In this section, please run the provided Python code, add the code needed to complete the tasks described below, and use the results to answer the questions in the HW assignment. Change your Runtime to TPU in order to speed up processing.\n",
        "\n",
        "\n",
        "\n",
        "**Task 1: Data Preparation**\n",
        "\n",
        "Load the Iris dataset.\n",
        "Preprocess the data: scaling between 0 and 1.\n",
        "Split the dataset into training and testing sets.\n",
        "\n",
        "(Run this code)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YmxzRqdQB6HU"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow import random\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Flatten, Reshape\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Load the MNIST dataset\n",
        "(X_train, _), (X_test, _) = mnist.load_data()\n",
        "\n",
        "# Normalize the data to a range between 0 and 1\n",
        "X_train = X_train.astype('float32') / 255.0\n",
        "X_test = X_test.astype('float32') / 255.0\n",
        "\n",
        "# Flatten the 28x28 images into vectors of size 784\n",
        "X_train = X_train.reshape((len(X_train), np.prod(X_train.shape[1:])))\n",
        "X_test = X_test.reshape((len(X_test), np.prod(X_test.shape[1:])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieR5LukLCBPz"
      },
      "source": [
        "**Task 2: Build the Autoencoder**\n",
        "\n",
        "**Coding Exercise**: Put in your code where the comment \"PUT YOUR CODE HERE\" is.\n",
        "\n",
        "Build an encoding layer, a decoding layer, and an autoencoder model.\n",
        "\n",
        "- Encoding layer: Create a dense (fully connected) layer with a specified number of neurons (encoding_dim) and use the ReLU (Rectified Linear Unit) activation function. Call this layer \"encoded\".\n",
        "- Specify that the encoded layer takes its input from input_layer.\n",
        "\n",
        "- Decoding layer: Create a dense layer with the same number of neurons as the input data's features (data.shape[1]) and use the sigmoid activation function. Call this layer \"decoded\".\n",
        "- Specify that the decoded layer receives input from the encoded layer.\n",
        "\n",
        "- Autoencoder model: Use the Model class to create the autoencoder model. Specify its input to the input_layer and its output to the decoded layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "flif9Dt_CGoj"
      },
      "outputs": [],
      "source": [
        "# Define the size of the encoded representation\n",
        "encoding_dim = 64  # Size of the encoded layer\n",
        "\n",
        "# Input layer\n",
        "input_layer = Input(shape=(X_train.shape[1],))\n",
        "\n",
        "\n",
        "# PUT YOUR CODE HERE vvvvv\n",
        "\n",
        "\n",
        "# Add encoding layer\n",
        "\n",
        "# Add decoding layer\n",
        "\n",
        "# Add autoencoder model\n",
        "\n",
        "\n",
        "# PUT YOUR CODE HERE ^^^^^\n",
        "\n",
        "\n",
        "# Compile the autoencoder model\n",
        "autoencoder.compile(optimizer='adam', loss='mse')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3FPUhbLCL1t"
      },
      "source": [
        "**Task 3: Train the Model**\n",
        "\n",
        "Train the model on the training data.\n",
        "Evaluate the model on the test data.\n",
        "\n",
        "Run this code once you have completed Task 2 above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nk5zZSuYCQhs"
      },
      "outputs": [],
      "source": [
        "# Train the autoencoder\n",
        "history = autoencoder.fit(X_train, X_train,\n",
        "                          epochs=10,\n",
        "                          batch_size=256,\n",
        "                          shuffle=True,\n",
        "                          validation_data=(X_test, X_test),\n",
        "                          verbose=1)\n",
        "\n",
        "# Encode and decode some test data\n",
        "encoded_data = autoencoder.predict(X_test)\n",
        "loss = autoencoder.evaluate(X_test, X_test, verbose=0)\n",
        "\n",
        "# Print the final loss\n",
        "print(f\"Final loss on test data: {loss:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}