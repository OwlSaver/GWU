{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Homework Assignment #6: LSTM and Recurrent Neural Networks (RNNs)"
      ],
      "metadata": {
        "id": "BGVq6H8P8Tez"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1: Recurrent Neural Networks\n",
        "\n",
        "**Implementing an RNN for Sentiment Analysis**\n",
        "\n",
        "In this assignment, you will implement a simple Recurrent Neural Network (RNN) for sentiment analysis using the IMDB movie reviews dataset. You will build an RNN model using TensorFlow/Keras to classify movie reviews as positive or negative.\n",
        "\n",
        "\n",
        "Fill in the code if indicated with the comment \"PUT YOUR CODE HERE\" and follow all the steps in the document.\n",
        "\n",
        "In this section, please run the provided Python code, add the code needed to complete the tasks described below, and use the results to answer the questions in the HW assignment. Change your Runtime to TPU in order to speed up processing.\n",
        "\n",
        "\n",
        "\n",
        "**Task 1: Data Preparation**\n",
        "\n",
        "Load the IMDB movie reviews dataset.\n",
        "Preprocess the text data: tokenization, padding sequences.\n",
        "\n",
        "(Run this code)\n"
      ],
      "metadata": {
        "id": "pT70YGx1B4B6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Load IMDB dataset\n",
        "num_words = 10000\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=num_words)\n",
        "\n",
        "# Preprocess data\n",
        "max_len = 200\n",
        "x_train = pad_sequences(x_train, maxlen=max_len)\n",
        "x_test = pad_sequences(x_test, maxlen=max_len)\n"
      ],
      "metadata": {
        "id": "YmxzRqdQB6HU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2: Build the RNN Model**\n",
        "\n",
        "**Coding Exercise**: Put in your code where the comment \"PUT YOUR CODE HERE\" is.\n",
        "\n",
        "Build a sequential model with an Embedding layer, an RNN layer, and a Dense layer.\n",
        "Compile the model with appropriate loss and optimizer.\n",
        "\n",
        "- Define model\n",
        "- Create a sequential model, which is a linear stack of layers.\n",
        "- Add and Embedding layer to the model. This layer converts input sequences of integers (each representing a word index) into dense vectors of fixed size. Use num_words to specify the size of the vocabulary, 32 is the dimensionality of the embedding space, and set the maximum input sequence length to input_length=max_len.\n",
        "- Add a simple RNN layer to the model. This layer implements the basic RNN cell. 32 specifies the number of units (hidden states) in the RNN layer.\n",
        "- Add a Dense layer with a single unit and a sigmoid activation function. This layer is the output layer of the model."
      ],
      "metadata": {
        "id": "ieR5LukLCBPz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
        "\n",
        "\n",
        "# PUT YOUR CODE HERE vvvvv\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# PUT YOUR CODE HERE ^^^^^\n",
        "\n",
        "#---------------------------------------------\n",
        "# Compile model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "flif9Dt_CGoj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 3: Train the Model**\n",
        "\n",
        "Train the model on the training data.\n",
        "Evaluate the model on the test data.\n",
        "\n",
        "Run this code once you have completed Task 2 above."
      ],
      "metadata": {
        "id": "h3FPUhbLCL1t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train model\n",
        "history = model.fit(x_train, y_train, epochs=5, batch_size=128, validation_split=0.2)\n",
        "\n",
        "# Evaluate model\n",
        "loss, accuracy = model.evaluate(x_test, y_test)\n",
        "print(\"Test Accuracy:\", accuracy)"
      ],
      "metadata": {
        "id": "Nk5zZSuYCQhs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 4: Fill in the Code (Coding Exercise)**\n",
        "\n",
        "Fill in the code marked with # PUT YOUR CODE HERE to make predictions on new reviews.\n",
        "\n",
        "- Create a new variable called \"prediction\".\n",
        "- Set that variable equal to the model.predict function, and run that function on review_pad.\n",
        "- Write an if/else statement that creates the following logic\n",
        "  - If prediction is greater than 0.5 then return \"Positive\"\n",
        "  - Otherwise return \"Negative\""
      ],
      "metadata": {
        "id": "2B1SqI0LCVwm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define tokenizer\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer(num_words=num_words)\n",
        "\n",
        "# Convert sequences back to text\n",
        "texts_train = tokenizer.sequences_to_texts(x_train)\n",
        "\n",
        "# Fit tokenizer on texts\n",
        "tokenizer.fit_on_texts(texts_train)\n",
        "\n",
        "def predict_sentiment(review):\n",
        "    # Tokenize and pad the input sequence\n",
        "    review_seq = tokenizer.texts_to_sequences([review])\n",
        "    review_pad = pad_sequences(review_seq, maxlen=max_len)\n",
        "\n",
        "# PUT YOUR CODE HERE vvvvv\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# PUT YOUR CODE HERE ^^^^^\n",
        "\n",
        "#*********************\n",
        "\n",
        "# Example usage\n",
        "review = \"This movie was fantastic! I loved every moment of it.\"\n",
        "#review = \"This movie was terrible! I was bored out of my mind.\"\n",
        "#review = \"Two thumbs down, unoriginal and pedantic! Numbingly predictable.\"\n",
        "#review = \"Absolutely spellbinding and thrilling! Oppenheimer is a movie for the generations!\"\n",
        "print(predict_sentiment(review))"
      ],
      "metadata": {
        "id": "EYZAdo4yCg3k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2: LSTM Implementation\n",
        "Fill in the code marked with # PUT YOUR CODE HERE.\n",
        "\n",
        "- Add an LSTM layer with 32 units.\n",
        "- Add a dense layer with 256 units. Use the \"relu\" activation function for this dense layer.\n"
      ],
      "metadata": {
        "id": "C2crL5mu6MQ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import imdb\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM, Flatten, Dropout\n",
        "from keras.layers import Embedding\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "num_words = 2000\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(path=\"imdb.npz\",\n",
        "                                                      num_words=num_words,\n",
        "                                                      skip_top=0,\n",
        "                                                      maxlen=None,\n",
        "                                                      seed=113,\n",
        "                                                      start_char=1,\n",
        "                                                      oov_char=2,\n",
        "                                                      index_from=3)\n",
        "max_review_length = 250\n",
        "X_train = pad_sequences(X_train, maxlen=max_review_length)\n",
        "X_test = pad_sequences(X_test, maxlen=max_review_length)\n",
        "\n",
        "embedding_vector_length = 32\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=num_words, output_dim=embedding_vector_length, input_length=max_review_length))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "# PUT YOUR CODE HERE vvvvv\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# PUT YOUR CODE HERE ^^^^^\n",
        "\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(units=1, activation='sigmoid'))\n",
        "model.summary()\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "train_history = model.fit(X_train, y_train, batch_size=32,\n",
        "                          epochs=10, verbose=2,\n",
        "                          validation_split=0.2)\n",
        "scores = model.evaluate(X_test, y_test, verbose=1) #scores[1] is the accuracy\n",
        "scores[1]"
      ],
      "metadata": {
        "id": "05KMlE5m6UsF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}