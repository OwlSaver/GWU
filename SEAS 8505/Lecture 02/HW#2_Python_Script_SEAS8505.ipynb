{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Homework #2: Decision Trees\n",
        "\n",
        "In this assignment, you will learn how to develop your own Decision tree models in Python. We will also compare and contrast entropy with the Gini Impurity score.\n",
        "\n",
        "In this section, please run the provided Python code, add the code needed to complete the tasks described below, and use the results to answer the questions in the HW assignment."
      ],
      "metadata": {
        "id": "E-1Mup6a9mDo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Part 1: Python Implementation of a Decision Tree.\n",
        "\n",
        "First implement a decision tree in Python using the sklearn package.\n",
        "\n",
        "Hint: Go to the following tutorial: https://scikit-learn.org/stable/modules/tree.html\n",
        "\n",
        "You need to implement something very similar.\n",
        "\n",
        "Implement the decision tree using the sklearn.tree() function and plot your decision tree."
      ],
      "metadata": {
        "id": "91jC6Pl2-ENf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Programming Question 1**\n",
        "\n",
        "Implement a Decision tree using the entropy criterion. Plot the decision tree and also create a confusion matrix for the decision tree.\n",
        "\n",
        "*Note:* Name your decision tree classifier clf1 for the later graphing visualization stage."
      ],
      "metadata": {
        "id": "fi8LYnM0B-hO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#First let's create the dataset. This dataset will be used for both decision trees\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn import tree\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "\n",
        "breast_cancer = load_breast_cancer()\n",
        "X, y = breast_cancer.data, breast_cancer.target\n",
        "\n",
        "#splitting the data into train, test values\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42)\n",
        "\n",
        "#Hint: you need y_pred for the confusion matrix which you can find by using the following: y_pred = clf1.predict(X_test)\n",
        "\n"
      ],
      "metadata": {
        "id": "24PvsfSiB6oK"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add your code here. Comments are for guidance."
      ],
      "metadata": {
        "id": "3k89s3ua0Pq0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#your code for the entropy decision tree goes here\n",
        "#See the tutorial for help.\n",
        "#change criterion to entropy\n",
        "\n",
        "#create a decision tree classifier using entropy classifier\n",
        "\n",
        "#plot the decision tree\n",
        "\n",
        "#generate and print a confusion matrix\n",
        "\n",
        "#name the classifier clf1 for the later graphing step\n",
        "\n",
        "# v***** WRITE YOUR CODE HERE *****v #\n",
        "\n",
        "\n",
        "# ^***** WRITE YOUR CODE HERE *****^ #\n"
      ],
      "metadata": {
        "id": "-LVNjuOoMUJ8"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Programming Question 2**\n",
        "\n",
        "Implement a decision tree using the sklearn package using the 'gini' criterion.\n",
        "Plot your decision tree, and create a confusion matrix for the decision tree.\n",
        "\n",
        "*Note:* Name your decision tree classifier clf2 for later graphing and visualization."
      ],
      "metadata": {
        "id": "3gTEboxbCdmz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#your code for the gini decision tree goes here\n",
        "#See the tutorial for help.\n",
        "#change criterion to gini\n",
        "#name the classifier clf2 for the later graphing steps\n",
        "\n",
        "\n",
        "#create a decision tree using the gini criterion\n",
        "#plot the decison tree\n",
        "#generate a confusion matrix for the decision tree\n",
        "\n",
        "# v***** WRITE YOUR CODE HERE *****v #\n",
        "\n",
        "\n",
        "# ^***** WRITE YOUR CODE HERE *****^ #\n"
      ],
      "metadata": {
        "id": "Lh1gumqSCv9O"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Part 2: Plotting Impurity Measures\n"
      ],
      "metadata": {
        "id": "V3rVK1RU0r1g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 3** Plot Impurity Measures. Follow the instructions below.\n",
        "\n",
        "The following Python code defines functions to calculate different impurity measures commonly used in decision tree algorithms: Gini impurity, entropy, and misclassification error. Then, it generates a plot comparing these measures for different values of the probability parameter p(j=1), which represents the probability of a binary outcome being in class 1. This code essentially demonstrates how different impurity measures behave as the probability of a binary outcome changes.Based on the Lecture, please complete the missing code. Fill in the missing code and answer the questions that follow:\n",
        "\n"
      ],
      "metadata": {
        "id": "uBdxeVtZM7kS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Bring in requsite libraries matplotlib for plotting and numpy for scientific computing\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "# Defining Impurity Measures Functions (this is the part you need to complete)\n",
        "def gini(p):\n",
        "# replace \"(p)\" with the formula to calculate gini for a given probability p\n",
        "\n",
        "# v***** WRITE YOUR CODE HERE *****v #\n",
        "  return p\n",
        "# ^***** WRITE YOUR CODE HERE *****^ #\n",
        "\n",
        "def entropy(p):\n",
        "# replace \"(p)\" with the formula to calculate entropy for a given probability p\n",
        "\n",
        "# v***** WRITE YOUR CODE HERE *****v #\n",
        "  return (p)\n",
        "# ^***** WRITE YOUR CODE HERE *****^ #\n",
        "\n",
        "\n",
        "def classification_error(p):\n",
        "# replace \"(p)\" with the formula to calculate classification error for a given probability p\n",
        "\n",
        "# v***** WRITE YOUR CODE HERE *****v #\n",
        "  return (p)\n",
        "# ^***** WRITE YOUR CODE HERE *****^ #\n",
        "\n",
        "\n",
        "# Generating Data:\n",
        "# Create an array of probabilities ranging from 0 to 1 with a step size of 0.01\n",
        "x = np.arange(0.0, 1.0, 0.01)\n",
        "\n",
        "# Calculate entropy for each value of x. It returns None if p is 0 to avoid a math domain error.\n",
        "ent = [entropy(p) if p != 0 else None for p in x]\n",
        "\n",
        "# Scales the entropy values by 0.5. It returns None if the entropy value is None.\n",
        "scaled_ent = [e*0.5 if e else None for e in ent]\n",
        "\n",
        "#Calculate the misclassification error for each value of x.\n",
        "c_err = [classification_error(i) for i in x]\n",
        "\n",
        "#Plotting:\n",
        "\n",
        "#create a figure object\n",
        "fig = plt.figure()\n",
        "\n",
        "#create a subplot\n",
        "ax = plt.subplot(111)\n",
        "\n",
        "#A loop iterates over different impurity measures and plots them:\n",
        "#-It iterates over a list containing the functions for entropy, scaled entropy, Gini impurity, and misclassification error.\n",
        "#-Each impurity measure is plotted against the range of probabilities x.\n",
        "#-->lab contains the label for each plot.\n",
        "#-->ls contains the linestyle for each plot.\n",
        "#-->c contains the color for each plot.\n",
        "for j, lab, ls, c, in zip(\n",
        "      [ent, scaled_ent, gini(x), c_err],\n",
        "      ['Entropy', 'Entropy (scaled)', 'Gini Impurity', 'Misclassification Error'],\n",
        "      ['-', '-', '--', '-.'],\n",
        "      ['lightgray', 'red', 'green', 'blue']):\n",
        "   line = ax.plot(x, j, label=lab, linestyle=ls, lw=1, color=c)\n",
        "\n",
        "#Customization\n",
        "\n",
        "#Add a legend to the plot:\n",
        "ax.legend(loc='upper left', bbox_to_anchor=(0.01, 0.85),\n",
        "         ncol=1, fancybox=True, shadow=False)\n",
        "ax.axhline(y=0.5, linewidth=1, color='k', linestyle='--')\n",
        "ax.axhline(y=1.0, linewidth=1, color='k', linestyle='--')\n",
        "\n",
        "#Set the y-axis limits:\n",
        "plt.ylim([0, 1.1])\n",
        "\n",
        "#Set the labels for the x-axis and y-axis:\n",
        "plt.xlabel('p(j=1)')\n",
        "plt.ylabel('Impurity Index')"
      ],
      "metadata": {
        "id": "MC5L1Dkl_e3U"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}