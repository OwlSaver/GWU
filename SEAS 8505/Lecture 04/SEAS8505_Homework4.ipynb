{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Homework #4: Random Forests and Ensembles\n",
        "\n",
        "In this assignment, you will develop your own ensemble learning models\n",
        "\n",
        "Fill in the code if indicated with the comment \"PUT YOUR CODE HERE\" and follow all the steps in the document.\n",
        "\n",
        "In this section, please run the provided Python code, add the code needed to complete the tasks described below, and use the results to answer the questions in the HW assignment."
      ],
      "metadata": {
        "id": "E-1Mup6a9mDo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "wubmJ_WJ_JhL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1: Implementing Ensemble Methods\n",
        "\n"
      ],
      "metadata": {
        "id": "lOqBGYKzDaSU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Objective**:\n",
        "The objective of this assignment is to explore and compare the performance of three popular ensemble learning algorithms: Random Forest, AdaBoost, and XGBoost, using a dataset. You will analyze the decision boundaries of each model and discuss the differences in their performance.\n",
        "\n",
        "**Dataset**:\n",
        "You will use the Iris dataset, a classic dataset in machine learning and statistics. The Iris dataset contains 150 samples of iris flowers, each with four features: sepal length, sepal width, petal length, and petal width. The task is to classify each sample into one of three species: setosa, versicolor, or virginica.\n",
        "\n",
        "**Tasks**:\n",
        "\n",
        "\n",
        "1. Train three ensemble learning models: Random Forest, AdaBoost, and XGBoost, using the training set. Make sure to use n_estimators=100, random_state=42 for all three models.\n"
      ],
      "metadata": {
        "id": "E0Ry0vKiIvhV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from mlxtend.plotting import plot_decision_regions\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data[:, :2], iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Random Forest classifier - Your Code Goes Here (find rf_clf)\n",
        "\n",
        "\n",
        "# Train AdaBoost classifier - Your Code Goes Here (find adaboost_clf)\n",
        "\n",
        "\n",
        "# Train XGBoost classifier - Your Code Goes Here (find xgb_clf)\n",
        "\n",
        "\n",
        "# Plot decision boundaries for Random Forest\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 3, 1)\n",
        "plot_decision_regions(X_train, y_train, clf=rf_clf, legend=2)\n",
        "plt.title('Random Forest Decision Boundaries')\n",
        "\n",
        "# Plot decision boundaries for AdaBoost\n",
        "plt.subplot(1, 3, 2)\n",
        "plot_decision_regions(X_train, y_train, clf=adaboost_clf, legend=2)\n",
        "plt.title('AdaBoost Decision Boundaries')\n",
        "\n",
        "# Plot decision boundaries for XGBoost\n",
        "plt.subplot(1, 3, 3)\n",
        "plot_decision_regions(X_train, y_train, clf=xgb_clf, legend=2)\n",
        "plt.title('XGBoost Decision Boundaries')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Evaluate model accuracies on the testing set\n",
        "rf_acc = rf_clf.score(X_test, y_test)\n",
        "adaboost_acc = adaboost_clf.score(X_test, y_test)\n",
        "xgb_acc = xgb_clf.score(X_test, y_test)\n",
        "\n",
        "print(\"Random Forest Accuracy:\", rf_acc)\n",
        "print(\"AdaBoost Accuracy:\", adaboost_acc)\n",
        "print(\"XGBoost Accuracy:\", xgb_acc)"
      ],
      "metadata": {
        "id": "bQr7Jz0-Iy8P",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
        "\n",
        "# Evaluate model performance on the testing set\n",
        "def evaluate_model_performance(clf, X_test, y_test):\n",
        "    y_pred = clf.predict(X_test)\n",
        "    precision = precision_score(y_test, y_pred, average='weighted')\n",
        "    recall = recall_score(y_test, y_pred, average='weighted')\n",
        "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    return precision, recall, f1, cm\n",
        "\n",
        "# Evaluate model performance for Random Forest\n",
        "rf_precision, rf_recall, rf_f1, rf_cm = evaluate_model_performance(rf_clf, X_test, y_test)\n",
        "print(\"Random Forest Metrics:\")\n",
        "print(\"Precision:\", rf_precision)\n",
        "print(\"Recall:\", rf_recall)\n",
        "print(\"F1-Score:\", rf_f1)\n",
        "print(\"Confusion Matrix:\\n\", rf_cm)\n",
        "\n",
        "# Evaluate model performance for AdaBoost\n",
        "adaboost_precision, adaboost_recall, adaboost_f1, adaboost_cm = evaluate_model_performance(adaboost_clf, X_test, y_test)\n",
        "print(\"\\nAdaBoost Metrics:\")\n",
        "print(\"Precision:\", adaboost_precision)\n",
        "print(\"Recall:\", adaboost_recall)\n",
        "print(\"F1-Score:\", adaboost_f1)\n",
        "print(\"Confusion Matrix:\\n\", adaboost_cm)\n",
        "\n",
        "# Evaluate model performance for XGBoost\n",
        "xgb_precision, xgb_recall, xgb_f1, xgb_cm = evaluate_model_performance(xgb_clf, X_test, y_test)\n",
        "print(\"\\nXGBoost Metrics:\")\n",
        "print(\"Precision:\", xgb_precision)\n",
        "print(\"Recall:\", xgb_recall)\n",
        "print(\"F1-Score:\", xgb_f1)\n",
        "print(\"Confusion Matrix:\\n\", xgb_cm)"
      ],
      "metadata": {
        "id": "nWG0PpeiJYcR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}